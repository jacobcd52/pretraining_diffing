{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from buffer import MultiModelActivationBuffer\n",
    "\n",
    "from datasets import load_dataset\n",
    "import torch as t\n",
    "from dataclasses import dataclass\n",
    "\n",
    "from nnsight import LanguageModel\n",
    "from buffer import MultiModelActivationBuffer\n",
    "from trainers.top_k import TopKTrainer, AutoEncoderTopK\n",
    "from training import trainSAE\n",
    "from einops import rearrange, einsum, repeat\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from trainers.top_k import AutoEncoderTopK\n",
    "device = \"cuda:0\"\n",
    "dtype = t.bfloat16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/pretraining_diffing/notebooks/../trainers/top_k.py:144: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = t.load(path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing statistics for rescaling activations...\n",
      "\n",
      "Batch 1/1\n",
      "Total samples so far: 65024\n",
      "\n",
      "Computing final statistics...\n",
      "Statistics computed.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "ae = AutoEncoderTopK.from_hf(f\"jacobcd52/pythia70m_step143000_sae\", device=device)\n",
    "\n",
    "layer = 4\n",
    "out_batch_size = 8192\n",
    "\n",
    "model_list = []\n",
    "submodule_list = []\n",
    "\n",
    "for step in [143000]: #tqdm([0, 128, 256, 512, 1000, 2000, 4000, 8000, 16000, 32000, 64000, 143000]):\n",
    "    model = LanguageModel(\n",
    "        \"EleutherAI/pythia-70m\", \n",
    "        revision=f\"step{step}\", \n",
    "        trust_remote_code=False, \n",
    "        device_map=device,\n",
    "        torch_dtype=dtype,\n",
    "        dispatch=True\n",
    "        )\n",
    "    for x in model.parameters():\n",
    "        x.requires_grad = False\n",
    "\n",
    "    model_list.append(model)\n",
    "    submodule_list.append(model.gpt_neox.layers[layer])\n",
    "    \n",
    "activation_dim = 512\n",
    "\n",
    "dataset = load_dataset(\n",
    "    'Skylion007/openwebtext', \n",
    "    split='train', \n",
    "    streaming=True,\n",
    "    trust_remote_code=True\n",
    "    )\n",
    "\n",
    "dataset = dataset.shuffle(42)\n",
    "\n",
    "class CustomData():\n",
    "    def __init__(self, dataset):\n",
    "        self.data = iter(dataset)\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        return next(self.data)['text']\n",
    "\n",
    "data = CustomData(dataset)\n",
    "\n",
    "buffer = MultiModelActivationBuffer(\n",
    "    data=CustomData(dataset),\n",
    "    model_list=model_list,\n",
    "    submodule_list=submodule_list,\n",
    "    d_submodule=activation_dim,\n",
    "    n_ctxs=1024, \n",
    "    device=device,\n",
    "    refresh_batch_size=512,\n",
    "    out_batch_size=out_batch_size,\n",
    "    remove_bos=True,\n",
    "    rescale_acts=True,\n",
    "    n_init_batches=1\n",
    ")\n",
    "\n",
    "buffer.act_mean[-1] = ae.act_mean\n",
    "buffer.act_cov_inv_sqrt[-1] = ae.act_cov_inv_sqrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Initialize buffer and autoencoder\\nbuffer = ...  # Your buffer that returns concatenated tensors\\nae = ...  # Your autoencoder model\\n\\n# Create configuration\\nconfig = Config()\\n\\n# Stage 1: Collect and save data\\ndata_info = collect_and_save_data(buffer, ae, config, chunk_size=10000)\\n\\n# Stage 2: Train probes using saved data\\nresults = train_probes_from_saved_data(config, data_info)\\n\\n# Analyze results\\nanalysis = evaluate_probes(results, config)\\n'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import os\n",
    "from dataclasses import dataclass, field\n",
    "from einops import rearrange\n",
    "\n",
    "# Configuration as a dataclass\n",
    "@dataclass\n",
    "class Config:\n",
    "    # Training parameters\n",
    "    learning_rate: float = 1e-3\n",
    "    num_epochs: int = 5\n",
    "    val_split: float = 0.2\n",
    "    \n",
    "    # Class weight parameter for addressing imbalance\n",
    "    pos_weight: float = 10.0  # Weight for positive class examples\n",
    "    \n",
    "    # Data collection\n",
    "    num_data_iterations: int = 100  # How many times to collect data\n",
    "    \n",
    "    # Paths\n",
    "    data_dir: str = \"collected_data/\"\n",
    "    save_dir: str = \"index_probes/\"\n",
    "    \n",
    "    # Device\n",
    "    device: torch.device = field(default_factory=lambda: torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "    \n",
    "    # Model parameters (to be set during runtime)\n",
    "    n_models: int = None  # Number of models (previously number of steps)\n",
    "    n_indices: int = None  # Will be set from ae.encoder.weight.shape[0]\n",
    "    k_indices: int = None  # Will be set from ae.k\n",
    "    batch_size: int = None  # Will be set from buffer output shape\n",
    "\n",
    "    # Evals\n",
    "    feature_chunk_size: int = 1000  # Number of features to process in each chunk   \n",
    "\n",
    "\n",
    "# Replace Focal Loss with a simple weighted BCE loss\n",
    "class WeightedBCEWithLogitsLoss(nn.Module):\n",
    "    def __init__(self, pos_weight=10.0, reduction='mean'):\n",
    "        super(WeightedBCEWithLogitsLoss, self).__init__()\n",
    "        self.pos_weight = pos_weight\n",
    "        self.reduction = reduction\n",
    "        \n",
    "    def forward(self, inputs, targets):\n",
    "        # Create a weight tensor that's 1.0 for negative class and pos_weight for positive class\n",
    "        weights = torch.ones_like(targets)\n",
    "        weights[targets > 0] = self.pos_weight\n",
    "        \n",
    "        # Apply weighted BCE loss\n",
    "        BCE_loss = nn.functional.binary_cross_entropy_with_logits(\n",
    "            inputs, targets, weight=weights, reduction=self.reduction\n",
    "        )\n",
    "        \n",
    "        return BCE_loss\n",
    "\n",
    "\n",
    "# Vectorized logistic classifier for all indices\n",
    "class VectorizedIndexProbe(nn.Module):\n",
    "    def __init__(self, input_dim, num_indices):\n",
    "        super(VectorizedIndexProbe, self).__init__()\n",
    "        self.linear = nn.Linear(input_dim, num_indices)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Convert input to same dtype as model weights to avoid dtype mismatches\n",
    "        if x.dtype != self.linear.weight.dtype:\n",
    "            x = x.to(self.linear.weight.dtype)\n",
    "        return self.linear(x)\n",
    "\n",
    "\n",
    "# Prepare targets from indices\n",
    "def prepare_targets(inds, n_indices):\n",
    "    \"\"\"\n",
    "    Convert indices tensor to binary targets for each possible index\n",
    "    \n",
    "    Args:\n",
    "        inds: Tensor of shape [batch, k] containing indices present\n",
    "        n_indices: Total number of possible indices\n",
    "        \n",
    "    Returns:\n",
    "        targets: Binary tensor of shape [batch, n_indices]\n",
    "    \"\"\"\n",
    "    batch_size, k = inds.shape\n",
    "    targets = torch.zeros(batch_size, n_indices, dtype=torch.float32, device=inds.device)\n",
    "    \n",
    "    # For each batch, set the indices present to 1\n",
    "    for b in range(batch_size):\n",
    "        targets[b, inds[b]] = 1.0\n",
    "    \n",
    "    return targets\n",
    "\n",
    "\n",
    "# Stage 1: Collect data and save to disk\n",
    "def collect_and_save_data(buffer, ae, config, iterations=None, chunk_size=10000):\n",
    "    \"\"\"\n",
    "    Collect data from buffer and save to disk in chunks\n",
    "    \n",
    "    Args:\n",
    "        buffer: Buffer that returns concatenated tensors\n",
    "        ae: Autoencoder model used to get indices\n",
    "        config: Configuration object\n",
    "        iterations: Number of iterations to collect data (if None, use config value)\n",
    "        chunk_size: Maximum number of samples to save in each chunk\n",
    "        \n",
    "    Returns:\n",
    "        data_info: Dictionary with information about saved data\n",
    "    \"\"\"\n",
    "    # Create data directory if it doesn't exist\n",
    "    os.makedirs(config.data_dir, exist_ok=True)\n",
    "    \n",
    "    # Set n_indices and k_indices from the autoencoder model\n",
    "    config.n_indices = ae.encoder.weight.shape[0]  # N\n",
    "    config.k_indices = ae.k  # k\n",
    "    print(f\"Determined n_indices={config.n_indices}, k_indices={config.k_indices} from model\")\n",
    "    \n",
    "    # Use config.num_data_iterations if iterations is None\n",
    "    if iterations is None:\n",
    "        iterations = config.num_data_iterations\n",
    "    \n",
    "    print(f\"Collecting data for {iterations} iterations with chunk size {chunk_size}...\")\n",
    "    \n",
    "    # Determine the dimension from the autoencoder\n",
    "    d = ae.encoder.weight.shape[1]\n",
    "    \n",
    "    # Keep track of total samples and chunks\n",
    "    total_samples = 0\n",
    "    chunk_count = 0\n",
    "    \n",
    "    # Lists to store data for current chunk\n",
    "    chunk_x = []\n",
    "    chunk_inds = []\n",
    "    \n",
    "    # Lists to store chunk information\n",
    "    x_chunks = []\n",
    "    inds_chunks = []\n",
    "    targets_chunks = []\n",
    "    \n",
    "    # Collect data\n",
    "    for i in tqdm(range(iterations)):\n",
    "        # Get concatenated data from buffer\n",
    "        x_concat = next(buffer)\n",
    "        \n",
    "        # Determine batch size from first iteration if not already set\n",
    "        if config.batch_size is None:\n",
    "            config.batch_size = x_concat.shape[0]\n",
    "            print(f\"Determined batch_size={config.batch_size} from buffer output\")\n",
    "        \n",
    "        # Rearrange to get separate model outputs\n",
    "        # Determine n_models if not already set\n",
    "        if config.n_models is None:\n",
    "            config.n_models = x_concat.shape[1] // d\n",
    "            print(f\"Determined n_models={config.n_models} from buffer output\")\n",
    "        \n",
    "        x = rearrange(x_concat, '... (n d) -> ... n d', n=config.n_models, d=d)\n",
    "        \n",
    "        # Get indices from the last model\n",
    "        _, _, inds, _ = ae.encode(x[:, -1], return_topk=True)\n",
    "        \n",
    "        # Store data\n",
    "        chunk_x.append(x.detach().cpu())\n",
    "        chunk_inds.append(inds.detach().cpu())\n",
    "        \n",
    "        # Update total samples\n",
    "        batch_size = x.shape[0]\n",
    "        total_samples += batch_size\n",
    "        \n",
    "        # Check if we should save this chunk\n",
    "        if total_samples >= chunk_size:\n",
    "            # Concatenate data for this chunk\n",
    "            x_data = torch.cat(chunk_x, dim=0)\n",
    "            inds_data = torch.cat(chunk_inds, dim=0)\n",
    "            \n",
    "            # Convert to float32 for consistency\n",
    "            if x_data.dtype != torch.float32:\n",
    "                x_data = x_data.to(torch.float32)\n",
    "            \n",
    "            # Prepare targets from indices\n",
    "            targets_data = prepare_targets(inds_data, config.n_indices)\n",
    "            \n",
    "            # Save chunk to disk\n",
    "            x_path = os.path.join(config.data_dir, f\"x_data_chunk{chunk_count}.pt\")\n",
    "            inds_path = os.path.join(config.data_dir, f\"indices_chunk{chunk_count}.pt\")\n",
    "            targets_path = os.path.join(config.data_dir, f\"targets_chunk{chunk_count}.pt\")\n",
    "            \n",
    "            torch.save(x_data, x_path)\n",
    "            torch.save(inds_data, inds_path)\n",
    "            torch.save(targets_data, targets_path)\n",
    "            \n",
    "            # Store chunk paths\n",
    "            x_chunks.append(x_path)\n",
    "            inds_chunks.append(inds_path)\n",
    "            targets_chunks.append(targets_path)\n",
    "            \n",
    "            # Reset for next chunk\n",
    "            chunk_x = []\n",
    "            chunk_inds = []\n",
    "            total_samples = 0\n",
    "            chunk_count += 1\n",
    "    \n",
    "    # Save any remaining data as a final chunk\n",
    "    if chunk_x:\n",
    "        # Concatenate remaining data\n",
    "        x_data = torch.cat(chunk_x, dim=0)\n",
    "        inds_data = torch.cat(chunk_inds, dim=0)\n",
    "        \n",
    "        # Convert to float32 for consistency\n",
    "        if x_data.dtype != torch.float32:\n",
    "            x_data = x_data.to(torch.float32)\n",
    "        \n",
    "        # Prepare targets from indices\n",
    "        targets_data = prepare_targets(inds_data, config.n_indices)\n",
    "        \n",
    "        # Save final chunk to disk\n",
    "        x_path = os.path.join(config.data_dir, f\"x_data_chunk{chunk_count}.pt\")\n",
    "        inds_path = os.path.join(config.data_dir, f\"indices_chunk{chunk_count}.pt\")\n",
    "        targets_path = os.path.join(config.data_dir, f\"targets_chunk{chunk_count}.pt\")\n",
    "        \n",
    "        torch.save(x_data, x_path)\n",
    "        torch.save(inds_data, inds_path)\n",
    "        torch.save(targets_data, targets_path)\n",
    "        \n",
    "        # Store chunk paths\n",
    "        x_chunks.append(x_path)\n",
    "        inds_chunks.append(inds_path)\n",
    "        targets_chunks.append(targets_path)\n",
    "    \n",
    "    # Save config and chunk information\n",
    "    data_info = {\n",
    "        \"n_models\": config.n_models,\n",
    "        \"n_indices\": config.n_indices,\n",
    "        \"k_indices\": config.k_indices,\n",
    "        \"batch_size\": config.batch_size,\n",
    "        \"total_chunks\": chunk_count + 1,\n",
    "        \"x_chunks\": x_chunks,\n",
    "        \"indices_chunks\": inds_chunks,\n",
    "        \"targets_chunks\": targets_chunks\n",
    "    }\n",
    "    \n",
    "    # Save data info to disk\n",
    "    info_path = os.path.join(config.data_dir, \"data_info.pkl\")\n",
    "    with open(info_path, \"wb\") as f:\n",
    "        pickle.dump(data_info, f)\n",
    "    \n",
    "    print(f\"Data collection complete. Total chunks: {chunk_count + 1}\")\n",
    "    \n",
    "    # Return data info\n",
    "    return data_info\n",
    "\n",
    "\n",
    "# Stage 2: Train probes using collected data\n",
    "def train_probes_from_saved_data(config, data_info=None):\n",
    "    \"\"\"\n",
    "    Train probes using data loaded from disk\n",
    "    \n",
    "    Args:\n",
    "        config: Configuration object\n",
    "        data_info: Dictionary with information about saved data (if None, load from default path)\n",
    "        \n",
    "    Returns:\n",
    "        results: Dictionary with probe results\n",
    "    \"\"\"\n",
    "    # Create save directory if it doesn't exist\n",
    "    os.makedirs(config.save_dir, exist_ok=True)\n",
    "    \n",
    "    # Load data info if not provided\n",
    "    if data_info is None:\n",
    "        info_path = os.path.join(config.data_dir, \"data_info.pkl\")\n",
    "        with open(info_path, \"rb\") as f:\n",
    "            data_info = pickle.load(f)\n",
    "    \n",
    "    # Update config with data info\n",
    "    config.n_models = data_info[\"n_models\"]\n",
    "    config.n_indices = data_info[\"n_indices\"]\n",
    "    config.k_indices = data_info[\"k_indices\"]\n",
    "    config.batch_size = data_info[\"batch_size\"]\n",
    "    \n",
    "    print(f\"Loaded config: n_models={config.n_models}, n_indices={config.n_indices}, k_indices={config.k_indices}\")\n",
    "    print(f\"Training with {data_info['total_chunks']} chunks of data\")\n",
    "    \n",
    "    # Results will be stored here\n",
    "    results = {}\n",
    "    \n",
    "    # Train a probe for each model\n",
    "    for model_idx in range(config.n_models):\n",
    "        print(f\"Training probe for model {model_idx}/{config.n_models-1}...\")\n",
    "        \n",
    "        # Train the vectorized probe for this model using all chunks\n",
    "        probe, auroc_scores, class_accuracies = train_model_probe_from_chunks(\n",
    "            model_idx, \n",
    "            data_info[\"x_chunks\"], \n",
    "            data_info[\"targets_chunks\"],\n",
    "            config\n",
    "        )\n",
    "        \n",
    "        # Store results\n",
    "        results[model_idx] = {\n",
    "            'probe': probe,\n",
    "            'auroc': auroc_scores,\n",
    "            'class_accuracies': class_accuracies\n",
    "        }\n",
    "        \n",
    "        # Calculate and print summary statistics\n",
    "        avg_auroc = np.mean(auroc_scores)\n",
    "        avg_pos_acc = np.nanmean(class_accuracies['positive'])\n",
    "        avg_neg_acc = np.nanmean(class_accuracies['negative'])\n",
    "        avg_balanced_acc = np.nanmean(class_accuracies['balanced'])\n",
    "        \n",
    "        print(f\"Model {model_idx} - Summary Statistics:\")\n",
    "        print(f\"  Average AUROC: {avg_auroc:.4f}\")\n",
    "        print(f\"  Average Positive Class Accuracy: {avg_pos_acc:.4f}\")\n",
    "        print(f\"  Average Negative Class Accuracy: {avg_neg_acc:.4f}\")\n",
    "        print(f\"  Average Balanced Accuracy: {avg_balanced_acc:.4f}\")\n",
    "        \n",
    "        # Save probe\n",
    "        torch.save(probe.state_dict(), f\"{config.save_dir}/vectorized_probe_model{model_idx}.pt\")\n",
    "        \n",
    "        # Save metrics\n",
    "        np.save(f\"{config.save_dir}/auroc_model{model_idx}.npy\", auroc_scores)\n",
    "        np.save(f\"{config.save_dir}/pos_acc_model{model_idx}.npy\", class_accuracies['positive'])\n",
    "        np.save(f\"{config.save_dir}/neg_acc_model{model_idx}.npy\", class_accuracies['negative'])\n",
    "        np.save(f\"{config.save_dir}/balanced_acc_model{model_idx}.npy\", class_accuracies['balanced'])\n",
    "    \n",
    "    # Save overall results\n",
    "    with open(f\"{config.save_dir}/results_summary.pkl\", \"wb\") as f:\n",
    "        pickle.dump({\n",
    "            model_idx: {\n",
    "                'auroc': results[model_idx]['auroc'],\n",
    "                'pos_acc': results[model_idx]['class_accuracies']['positive'],\n",
    "                'neg_acc': results[model_idx]['class_accuracies']['negative'],\n",
    "                'balanced_acc': results[model_idx]['class_accuracies']['balanced']\n",
    "            } for model_idx in results\n",
    "        }, f)\n",
    "    \n",
    "    return results\n",
    "\n",
    "def train_model_probe_from_chunks(model_idx, x_chunk_paths, targets_chunk_paths, config):\n",
    "    \"\"\"\n",
    "    Train a probe for a specific model using chunked data\n",
    "    \n",
    "    Args:\n",
    "        model_idx: Index of the model to train probe for\n",
    "        x_chunk_paths: List of paths to x data chunks\n",
    "        targets_chunk_paths: List of paths to targets data chunks\n",
    "        config: Configuration object\n",
    "        \n",
    "    Returns:\n",
    "        probe: Trained probe model\n",
    "        auroc_scores: AUROC scores for each index\n",
    "        class_accuracies: Dictionary with per-class accuracies\n",
    "        valid_indices: Boolean mask indicating which indices have enough positive examples\n",
    "    \"\"\"\n",
    "    # Import torcheval for AUROC calculation\n",
    "    from torcheval.metrics import BinaryAUROC\n",
    "    \n",
    "    # Initialize the model\n",
    "    d = ae.encoder.weight.shape[1]  # Get dimension from the autoencoder\n",
    "    probe = VectorizedIndexProbe(d, config.n_indices).to(config.device)\n",
    "    \n",
    "    # Initialize optimizer and loss\n",
    "    optimizer = optim.Adam(probe.parameters(), lr=config.learning_rate)\n",
    "    criterion = WeightedBCEWithLogitsLoss(pos_weight=config.pos_weight)\n",
    "    \n",
    "    # Set aside one chunk for validation (don't use the last chunk, use a random one)\n",
    "    num_chunks = len(x_chunk_paths)\n",
    "    if num_chunks > 1:\n",
    "        # Use a random chunk for validation, but not the last one which might be incomplete\n",
    "        val_chunk_idx = np.random.randint(0, num_chunks - 1)\n",
    "    else:\n",
    "        # If only one chunk, use part of it for validation\n",
    "        val_chunk_idx = None\n",
    "    \n",
    "    # Create training chunk indices (exclude validation chunk)\n",
    "    train_chunk_indices = [i for i in range(num_chunks) if i != val_chunk_idx]\n",
    "    \n",
    "    print(f\"Using chunk {val_chunk_idx} for validation, {len(train_chunk_indices)} chunks for training\")\n",
    "    \n",
    "    # Training loop - process chunks sequentially\n",
    "    for epoch in range(config.num_epochs):\n",
    "        probe.train()\n",
    "        epoch_loss = 0.0\n",
    "        total_batches = 0\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{config.num_epochs} - Processing {len(train_chunk_indices)} training chunks\")\n",
    "        \n",
    "        # Iterate through training chunks\n",
    "        for chunk_idx in train_chunk_indices:\n",
    "            # Load chunk data with weights_only=True\n",
    "            x_data = torch.load(x_chunk_paths[chunk_idx], weights_only=True)\n",
    "            targets_data = torch.load(targets_chunk_paths[chunk_idx], weights_only=True)\n",
    "            \n",
    "            # Extract features for this model\n",
    "            x_model = x_data[:, model_idx]\n",
    "            \n",
    "            # Create data loader for this chunk\n",
    "            chunk_dataset = TensorDataset(x_model, targets_data)\n",
    "            chunk_loader = DataLoader(chunk_dataset, batch_size=config.batch_size, shuffle=True)\n",
    "            \n",
    "            # Train on this chunk\n",
    "            chunk_loss = 0.0\n",
    "            for batch_x, batch_targets in chunk_loader:\n",
    "                batch_x = batch_x.to(config.device)\n",
    "                batch_targets = batch_targets.to(config.device)\n",
    "                \n",
    "                # Ensure data types match\n",
    "                if batch_x.dtype != torch.float32:\n",
    "                    batch_x = batch_x.to(torch.float32)\n",
    "                if batch_targets.dtype != torch.float32:\n",
    "                    batch_targets = batch_targets.to(torch.float32)\n",
    "                \n",
    "                # Forward pass\n",
    "                outputs = probe(batch_x)\n",
    "                loss = criterion(outputs, batch_targets)\n",
    "                \n",
    "                # Backward pass and optimize\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                chunk_loss += loss.item()\n",
    "                total_batches += 1\n",
    "            \n",
    "            # Report chunk progress\n",
    "            avg_chunk_loss = chunk_loss / len(chunk_loader)\n",
    "            epoch_loss += chunk_loss\n",
    "            print(f\"  Chunk {chunk_idx}/{len(train_chunk_indices)-1}, Loss: {avg_chunk_loss:.4f}\")\n",
    "        \n",
    "        # Report epoch results\n",
    "        avg_epoch_loss = epoch_loss / total_batches\n",
    "        print(f\"Epoch {epoch+1}/{config.num_epochs}, Avg Loss: {avg_epoch_loss:.4f}\")\n",
    "    \n",
    "    # Evaluate on validation set\n",
    "    probe.eval()\n",
    "    \n",
    "    # If we have a validation chunk, use it\n",
    "    if val_chunk_idx is not None:\n",
    "        # Load validation data with weights_only=True\n",
    "        x_val_data = torch.load(x_chunk_paths[val_chunk_idx], weights_only=True)\n",
    "        targets_val_data = torch.load(targets_chunk_paths[val_chunk_idx], weights_only=True)\n",
    "    else:\n",
    "        # Otherwise split the last chunk for validation\n",
    "        x_data = torch.load(x_chunk_paths[0], weights_only=True)\n",
    "        targets_data = torch.load(targets_chunk_paths[0], weights_only=True)\n",
    "        n_samples = x_data.shape[0]\n",
    "        n_val = int(n_samples * config.val_split)\n",
    "        \n",
    "        # Split the data\n",
    "        indices = torch.randperm(n_samples)\n",
    "        val_indices = indices[:n_val]\n",
    "        \n",
    "        x_val_data = x_data[val_indices]\n",
    "        targets_val_data = targets_data[val_indices]\n",
    "    \n",
    "    # Extract features for this model\n",
    "    x_val_model = x_val_data[:, model_idx]\n",
    "    \n",
    "    # Create data loader for validation with a smaller batch size to reduce memory usage\n",
    "    # Using a smaller batch size during evaluation to prevent memory issues\n",
    "    eval_batch_size = min(1024, config.batch_size)  # Limit batch size during evaluation\n",
    "    val_dataset = TensorDataset(x_val_model, targets_val_data)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=eval_batch_size, shuffle=False)\n",
    "    \n",
    "    # Identify indices with at least 10 positive examples in the validation set\n",
    "    # We do this BEFORE processing the whole validation set to save memory\n",
    "    pos_counts = torch.zeros(config.n_indices)\n",
    "    \n",
    "    # Count positives in batches to avoid loading all data at once\n",
    "    for _, batch_targets in val_loader:\n",
    "        pos_counts += torch.sum(batch_targets, dim=0)\n",
    "    \n",
    "    # Identify valid indices (at least 10 positive examples)\n",
    "    valid_indices = pos_counts >= 10\n",
    "    valid_proportion = torch.mean(valid_indices.float()).item()\n",
    "    valid_indices_list = torch.where(valid_indices)[0].tolist()\n",
    "    \n",
    "    print(f\"\\nProportion of indices with ≥10 positive examples in validation set: {valid_proportion:.4f} ({len(valid_indices_list)}/{config.n_indices})\")\n",
    "    \n",
    "    # Initialize AUROC scores tensor with default value 0.5\n",
    "    auroc_scores = np.ones(config.n_indices) * -1\n",
    "    \n",
    "    # Only compute metrics for valid indices to save memory\n",
    "    if valid_indices_list:\n",
    "        # Process valid indices in chunks to balance memory usage and efficiency\n",
    "        \n",
    "        for chunk_start in range(0, len(valid_indices_list), config.feature_chunk_size):\n",
    "            chunk_end = min(chunk_start + config.feature_chunk_size, len(valid_indices_list))\n",
    "            chunk_indices = valid_indices_list[chunk_start:chunk_end]\n",
    "            \n",
    "            print(f\"Processing AUROC for features {chunk_start} to {chunk_end-1} of {len(valid_indices_list)} valid features\")\n",
    "            \n",
    "            # Initialize metrics for this chunk of features\n",
    "            chunk_metrics = [BinaryAUROC() for _ in range(len(chunk_indices))]\n",
    "            \n",
    "            # Process each batch\n",
    "            for batch_x, batch_targets in val_loader:\n",
    "                batch_x = batch_x.to(config.device)\n",
    "                \n",
    "                # Forward pass through the model once per batch\n",
    "                with torch.no_grad():\n",
    "                    if batch_x.dtype != torch.float32:\n",
    "                        batch_x = batch_x.to(torch.float32)\n",
    "                    outputs = probe(batch_x)\n",
    "                    \n",
    "                    # Process each feature in this chunk\n",
    "                    for i, idx in enumerate(chunk_indices):\n",
    "                        # Apply sigmoid to get probabilities\n",
    "                        probs = torch.sigmoid(outputs[:, idx]).cpu()\n",
    "                        targets = batch_targets[:, idx].int()\n",
    "                        \n",
    "                        # Update metric with this batch\n",
    "                        chunk_metrics[i].update(probs.squeeze(), targets.squeeze())\n",
    "            \n",
    "            # Compute final AUROC for each feature in this chunk\n",
    "            for i, idx in enumerate(chunk_indices):\n",
    "                try:\n",
    "                    auroc_scores[idx] = chunk_metrics[i].compute().item()\n",
    "                except Exception as e:\n",
    "                    print(f\"Warning: Could not compute AUROC for index {idx}: {e}\")\n",
    "    \n",
    "    # Calculate per-class accuracies (with memory optimization)\n",
    "    pos_acc = np.zeros(config.n_indices)\n",
    "    neg_acc = np.zeros(config.n_indices)\n",
    "    balanced_acc = np.zeros(config.n_indices)\n",
    "    \n",
    "    # Initialize counters for each index\n",
    "    true_pos = np.zeros(config.n_indices)\n",
    "    false_pos = np.zeros(config.n_indices)\n",
    "    true_neg = np.zeros(config.n_indices)\n",
    "    false_neg = np.zeros(config.n_indices)\n",
    "    \n",
    "    # Process in batches\n",
    "    for batch_x, batch_targets in val_loader:\n",
    "        batch_x = batch_x.to(config.device)\n",
    "        \n",
    "        # Forward pass\n",
    "        with torch.no_grad():\n",
    "            if batch_x.dtype != torch.float32:\n",
    "                batch_x = batch_x.to(torch.float32)\n",
    "            outputs = probe(batch_x)\n",
    "            preds = (torch.sigmoid(outputs) > 0.5).float().cpu()\n",
    "            \n",
    "            # Update counters\n",
    "            batch_targets = batch_targets.cpu()\n",
    "            for idx in valid_indices_list:\n",
    "                # Get predictions and targets for this index\n",
    "                idx_preds = preds[:, idx].numpy()\n",
    "                idx_targets = batch_targets[:, idx].numpy()\n",
    "                \n",
    "                # Update confusion matrix\n",
    "                true_pos[idx] += np.sum((idx_preds == 1) & (idx_targets == 1))\n",
    "                false_pos[idx] += np.sum((idx_preds == 1) & (idx_targets == 0))\n",
    "                true_neg[idx] += np.sum((idx_preds == 0) & (idx_targets == 0))\n",
    "                false_neg[idx] += np.sum((idx_preds == 0) & (idx_targets == 1))\n",
    "    \n",
    "    # Calculate accuracies\n",
    "    for idx in valid_indices_list:\n",
    "        if (true_pos[idx] + false_neg[idx]) > 0:\n",
    "            pos_acc[idx] = true_pos[idx] / (true_pos[idx] + false_neg[idx])\n",
    "        else:\n",
    "            pos_acc[idx] = np.nan\n",
    "            \n",
    "        if (true_neg[idx] + false_pos[idx]) > 0:\n",
    "            neg_acc[idx] = true_neg[idx] / (true_neg[idx] + false_pos[idx])\n",
    "        else:\n",
    "            neg_acc[idx] = np.nan\n",
    "            \n",
    "        # Calculate balanced accuracy\n",
    "        valid_accs = []\n",
    "        if not np.isnan(pos_acc[idx]):\n",
    "            valid_accs.append(pos_acc[idx])\n",
    "        if not np.isnan(neg_acc[idx]):\n",
    "            valid_accs.append(neg_acc[idx])\n",
    "        \n",
    "        balanced_acc[idx] = np.mean(valid_accs) if valid_accs else np.nan\n",
    "    \n",
    "    # Store results\n",
    "    class_accuracies = {\n",
    "        'positive': pos_acc,\n",
    "        'negative': neg_acc,\n",
    "        'balanced': balanced_acc,\n",
    "        'valid_indices': valid_indices.cpu().numpy()\n",
    "    }\n",
    "    \n",
    "    # Report some examples of per-class accuracy\n",
    "    print(\"\\nClass accuracy examples (selected indices):\")\n",
    "    for idx in range(min(5, config.n_indices)):  # Show first 5 indices\n",
    "        pos_count = pos_counts[idx].item()\n",
    "        valid_status = \"valid\" if pos_count >= 10 else \"invalid\"\n",
    "        print(f\"  Index {idx}: Positive acc = {pos_acc[idx]:.4f}, Negative acc = {neg_acc[idx]:.4f}, \" \n",
    "              f\"Balanced acc = {balanced_acc[idx]:.4f}, Positive count = {pos_count} ({valid_status})\")\n",
    "    \n",
    "    return probe, auroc_scores, class_accuracies\n",
    "\n",
    "# Function to calculate class-specific accuracies\n",
    "def calculate_class_accuracies(probe, x, targets, config, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Calculate separate accuracies for positive and negative classes\n",
    "    \n",
    "    Args:\n",
    "        probe: Trained probe model\n",
    "        x: Input tensor\n",
    "        targets: Target tensor\n",
    "        config: Configuration\n",
    "        threshold: Decision threshold for binary classification\n",
    "        \n",
    "    Returns:\n",
    "        pos_accuracies: Accuracy for positive class (true positives / all positives)\n",
    "        neg_accuracies: Accuracy for negative class (true negatives / all negatives)\n",
    "        balanced_accuracies: Average of positive and negative accuracies\n",
    "    \"\"\"\n",
    "    probe.eval()\n",
    "    x = x.to(config.device)\n",
    "    targets = targets.to(config.device)\n",
    "    \n",
    "    # Ensure data types match\n",
    "    if x.dtype != torch.float32:\n",
    "        x = x.to(torch.float32)\n",
    "    \n",
    "    # Get predictions\n",
    "    with torch.no_grad():\n",
    "        logits = probe(x)\n",
    "        preds = (torch.sigmoid(logits) > threshold).float()\n",
    "    \n",
    "    # Calculate accuracies for each index\n",
    "    pos_accuracies = np.zeros(config.n_indices)\n",
    "    neg_accuracies = np.zeros(config.n_indices)\n",
    "    balanced_accuracies = np.zeros(config.n_indices)\n",
    "    \n",
    "    for idx in range(config.n_indices):\n",
    "        # Get predictions and targets for this index\n",
    "        idx_preds = preds[:, idx].cpu().numpy()\n",
    "        idx_targets = targets[:, idx].cpu().numpy()\n",
    "        \n",
    "        # Positive samples (where target is 1)\n",
    "        pos_mask = (idx_targets == 1)\n",
    "        if np.any(pos_mask):\n",
    "            pos_accuracies[idx] = np.mean(idx_preds[pos_mask] == idx_targets[pos_mask])\n",
    "        else:\n",
    "            pos_accuracies[idx] = np.nan\n",
    "        \n",
    "        # Negative samples (where target is 0)\n",
    "        neg_mask = (idx_targets == 0)\n",
    "        if np.any(neg_mask):\n",
    "            neg_accuracies[idx] = np.mean(idx_preds[neg_mask] == idx_targets[neg_mask])\n",
    "        else:\n",
    "            neg_accuracies[idx] = np.nan\n",
    "        \n",
    "        # Balanced accuracy (average of positive and negative accuracies)\n",
    "        valid_accs = []\n",
    "        if not np.isnan(pos_accuracies[idx]):\n",
    "            valid_accs.append(pos_accuracies[idx])\n",
    "        if not np.isnan(neg_accuracies[idx]):\n",
    "            valid_accs.append(neg_accuracies[idx])\n",
    "        \n",
    "        balanced_accuracies[idx] = np.mean(valid_accs) if valid_accs else np.nan\n",
    "    \n",
    "    return pos_accuracies, neg_accuracies, balanced_accuracies\n",
    "\n",
    "\n",
    "def evaluate_probes(results, config):\n",
    "    \"\"\"\n",
    "    Analyze and report on probe performance\n",
    "    \n",
    "    Args:\n",
    "        results: Dictionary with probe results for each model\n",
    "        config: Configuration object\n",
    "        \n",
    "    Returns:\n",
    "        analysis: Dictionary with analysis results\n",
    "    \"\"\"\n",
    "    analysis = {}\n",
    "    \n",
    "    for model_idx, model_results in results.items():\n",
    "        auroc_scores = model_results['auroc']\n",
    "        class_accuracies = model_results['class_accuracies']\n",
    "        valid_indices = class_accuracies['valid_indices']\n",
    "        \n",
    "        # Calculate statistics for AUROC - only for valid indices\n",
    "        valid_aurocs = auroc_scores[valid_indices]\n",
    "        mean_auroc = np.mean(valid_aurocs)\n",
    "        median_auroc = np.median(valid_aurocs)\n",
    "        min_auroc = np.min(valid_aurocs)\n",
    "        max_auroc = np.max(valid_aurocs)\n",
    "        \n",
    "        # Find top and bottom performing indices among valid ones\n",
    "        valid_indices_list = np.where(valid_indices)[0]\n",
    "        top_valid_indices = valid_indices_list[np.argsort(valid_aurocs)[::-1][:10]]\n",
    "        bottom_valid_indices = valid_indices_list[np.argsort(valid_aurocs)[:10]]\n",
    "        \n",
    "        print(f\"\\nModel {model_idx} Analysis:\")\n",
    "        print(f\"Proportion of indices with ≥10 positive examples: {np.mean(valid_indices):.4f} ({np.sum(valid_indices)}/{len(valid_indices)})\")\n",
    "        print(f\"Mean AUROC (valid indices): {mean_auroc:.4f}\")\n",
    "        print(f\"Median AUROC (valid indices): {median_auroc:.4f}\")\n",
    "        print(f\"Min AUROC (valid indices): {min_auroc:.4f}\")\n",
    "        print(f\"Max AUROC (valid indices): {max_auroc:.4f}\")\n",
    "        \n",
    "        # Calculate statistics for class accuracies - only for valid indices\n",
    "        pos_acc = class_accuracies['positive'][valid_indices]\n",
    "        neg_acc = class_accuracies['negative'][valid_indices]\n",
    "        balanced_acc = class_accuracies['balanced'][valid_indices]\n",
    "        \n",
    "        print(f\"Mean Positive Accuracy (valid indices): {np.nanmean(pos_acc):.4f}\")\n",
    "        print(f\"Mean Negative Accuracy (valid indices): {np.nanmean(neg_acc):.4f}\")\n",
    "        print(f\"Mean Balanced Accuracy (valid indices): {np.nanmean(balanced_acc):.4f}\")\n",
    "        \n",
    "        print(f\"\\nTop 10 valid indices by AUROC:\")\n",
    "        for i, idx in enumerate(top_valid_indices):\n",
    "            print(f\"  {i+1}. Index {idx}: AUROC = {auroc_scores[idx]:.4f}, \" \n",
    "                  f\"Pos Acc = {class_accuracies['positive'][idx]:.4f}, Neg Acc = {class_accuracies['negative'][idx]:.4f}\")\n",
    "        \n",
    "        print(f\"\\nBottom 10 valid indices by AUROC:\")\n",
    "        for i, idx in enumerate(bottom_valid_indices):\n",
    "            print(f\"  {i+1}. Index {idx}: AUROC = {auroc_scores[idx]:.4f}, \"\n",
    "                  f\"Pos Acc = {class_accuracies['positive'][idx]:.4f}, Neg Acc = {class_accuracies['negative'][idx]:.4f}\")\n",
    "        \n",
    "        # Store analysis\n",
    "        analysis[model_idx] = {\n",
    "            'mean_auroc': mean_auroc,\n",
    "            'median_auroc': median_auroc,\n",
    "            'min_auroc': min_auroc,\n",
    "            'max_auroc': max_auroc,\n",
    "            'top_indices': top_valid_indices.tolist(),\n",
    "            'bottom_indices': bottom_valid_indices.tolist(),\n",
    "            'mean_pos_acc': np.nanmean(pos_acc),\n",
    "            'mean_neg_acc': np.nanmean(neg_acc),\n",
    "            'mean_balanced_acc': np.nanmean(balanced_acc),\n",
    "            'valid_proportion': np.mean(valid_indices),\n",
    "            'valid_count': np.sum(valid_indices)\n",
    "        }\n",
    "    \n",
    "    # Save analysis results\n",
    "    analysis_path = os.path.join(config.save_dir, \"analysis_results.pkl\")\n",
    "    with open(analysis_path, \"wb\") as f:\n",
    "        pickle.dump(analysis, f)\n",
    "    \n",
    "    return analysis\n",
    "\n",
    "\n",
    "# Usage example\n",
    "\"\"\"\n",
    "# Initialize buffer and autoencoder\n",
    "buffer = ...  # Your buffer that returns concatenated tensors\n",
    "ae = ...  # Your autoencoder model\n",
    "\n",
    "# Create configuration\n",
    "config = Config()\n",
    "\n",
    "# Stage 1: Collect and save data\n",
    "data_info = collect_and_save_data(buffer, ae, config, chunk_size=10000)\n",
    "\n",
    "# Stage 2: Train probes using saved data\n",
    "results = train_probes_from_saved_data(config, data_info)\n",
    "\n",
    "# Analyze results\n",
    "analysis = evaluate_probes(results, config)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Config(\n",
    "    learning_rate=1e-1,\n",
    "    num_epochs=1,\n",
    "    val_split=0.2,\n",
    "    pos_weight=10.0,\n",
    "    num_data_iterations=200,\n",
    "    data_dir=\"collected_data/\",\n",
    "    save_dir=\"index_probes/\",\n",
    "    device=device,\n",
    "    batch_size=32768,\n",
    "    feature_chunk_size=1000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Determined n_indices=8192, k_indices=128 from model\n",
      "Collecting data for 200 iterations with chunk size 32768...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/200 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Determined n_models=1 from buffer output\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [01:58<00:00,  1.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data collection complete. Total chunks: 51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "path = collect_and_save_data(buffer, ae, config, chunk_size=32768)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded config: n_models=1, n_indices=8192, k_indices=128\n",
      "Training with 51 chunks of data\n",
      "Training probe for model 0/0...\n",
      "Using chunk 26 for validation, 49 chunks for training\n",
      "Epoch 1/1 - Processing 49 training chunks\n",
      "  Chunk 0/48, Loss: 0.8367\n",
      "  Chunk 1/48, Loss: 0.9236\n",
      "  Chunk 2/48, Loss: 0.8152\n",
      "  Chunk 3/48, Loss: 0.6937\n",
      "  Chunk 4/48, Loss: 0.6807\n",
      "  Chunk 5/48, Loss: 0.6398\n",
      "  Chunk 6/48, Loss: 0.5937\n",
      "  Chunk 7/48, Loss: 0.5296\n",
      "  Chunk 8/48, Loss: 0.4903\n",
      "  Chunk 9/48, Loss: 0.4681\n",
      "  Chunk 10/48, Loss: 0.4488\n",
      "  Chunk 11/48, Loss: 0.4275\n",
      "  Chunk 12/48, Loss: 0.3913\n",
      "  Chunk 13/48, Loss: 0.3677\n",
      "  Chunk 14/48, Loss: 0.3379\n",
      "  Chunk 15/48, Loss: 0.3175\n",
      "  Chunk 16/48, Loss: 0.3012\n",
      "  Chunk 17/48, Loss: 0.2855\n",
      "  Chunk 18/48, Loss: 0.2732\n",
      "  Chunk 19/48, Loss: 0.2553\n",
      "  Chunk 20/48, Loss: 0.2413\n",
      "  Chunk 21/48, Loss: 0.2274\n",
      "  Chunk 22/48, Loss: 0.2187\n",
      "  Chunk 23/48, Loss: 0.2077\n",
      "  Chunk 24/48, Loss: 0.1988\n",
      "  Chunk 25/48, Loss: 0.1898\n",
      "  Chunk 27/48, Loss: 0.1814\n",
      "  Chunk 28/48, Loss: 0.1739\n",
      "  Chunk 29/48, Loss: 0.1683\n",
      "  Chunk 30/48, Loss: 0.1614\n",
      "  Chunk 31/48, Loss: 0.1570\n",
      "  Chunk 32/48, Loss: 0.1511\n",
      "  Chunk 33/48, Loss: 0.1465\n",
      "  Chunk 34/48, Loss: 0.1417\n",
      "  Chunk 35/48, Loss: 0.1373\n",
      "  Chunk 36/48, Loss: 0.1336\n",
      "  Chunk 37/48, Loss: 0.1303\n",
      "  Chunk 38/48, Loss: 0.1273\n",
      "  Chunk 39/48, Loss: 0.1241\n",
      "  Chunk 40/48, Loss: 0.1214\n",
      "  Chunk 41/48, Loss: 0.1184\n",
      "  Chunk 42/48, Loss: 0.1162\n",
      "  Chunk 43/48, Loss: 0.1138\n",
      "  Chunk 44/48, Loss: 0.1119\n",
      "  Chunk 45/48, Loss: 0.1101\n",
      "  Chunk 46/48, Loss: 0.1079\n",
      "  Chunk 47/48, Loss: 0.1064\n",
      "  Chunk 48/48, Loss: 0.1043\n",
      "  Chunk 49/48, Loss: 0.1036\n",
      "Epoch 1/1, Avg Loss: 0.2941\n",
      "\n",
      "Proportion of indices with ≥10 positive examples in validation set: 0.9965 (8163/8192)\n",
      "Processing AUROC for features 0 to 999 of 8163 valid features\n",
      "Processing AUROC for features 1000 to 1999 of 8163 valid features\n",
      "Processing AUROC for features 2000 to 2999 of 8163 valid features\n",
      "Processing AUROC for features 3000 to 3999 of 8163 valid features\n",
      "Processing AUROC for features 4000 to 4999 of 8163 valid features\n",
      "Processing AUROC for features 5000 to 5999 of 8163 valid features\n",
      "Processing AUROC for features 6000 to 6999 of 8163 valid features\n",
      "Processing AUROC for features 7000 to 7999 of 8163 valid features\n",
      "Processing AUROC for features 8000 to 8162 of 8163 valid features\n",
      "\n",
      "Class accuracy examples (selected indices):\n",
      "  Index 0: Positive acc = 0.9136, Negative acc = 0.9657, Balanced acc = 0.9396, Positive count = 544.0 (valid)\n",
      "  Index 1: Positive acc = 0.4118, Negative acc = 0.9993, Balanced acc = 0.7055, Positive count = 34.0 (valid)\n",
      "  Index 2: Positive acc = 0.7097, Negative acc = 0.9988, Balanced acc = 0.8542, Positive count = 62.0 (valid)\n",
      "  Index 3: Positive acc = 0.8676, Negative acc = 0.9765, Balanced acc = 0.9221, Positive count = 408.0 (valid)\n",
      "  Index 4: Positive acc = 0.3908, Negative acc = 0.9981, Balanced acc = 0.6945, Positive count = 87.0 (valid)\n",
      "Model 0 - Summary Statistics:\n",
      "  Average AUROC: 0.9828\n",
      "  Average Positive Class Accuracy: 0.7039\n",
      "  Average Negative Class Accuracy: 0.9816\n",
      "  Average Balanced Accuracy: 0.8427\n"
     ]
    }
   ],
   "source": [
    "results = train_probes_from_saved_data(config, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg auroc 0.9828203137399287\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "aurocs = results[0]['auroc']\n",
    "good_aurocs = aurocs[aurocs > -1]\n",
    "print(\"avg auroc\", aurocs.mean())\n",
    "plt.hist(good_aurocs, bins=100)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
